{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "66181ca3",
   "metadata": {},
   "source": [
    "## Text Input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86eee9f0",
   "metadata": {},
   "source": [
    "https://platform.openai.com/docs/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c1464da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8862cbde",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    system_prompt=\"You are a science fiction writer, create a capital city at the users request.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a9e02c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this sci‑fi setting, the Moon’s capital is Selene Prime (also known as Selene City). It sits in Shackleton Crater at the lunar south pole, a hub of governance and culture for the Lunar Confederation. The city is ringed with glass-domed districts, ice-powered infrastructure, and solar-collector rings. The Lunar Assembly and Central Archive meet in the Council Dome, while a magnetically levitated transit network carries people and goods between districts.\n"
     ]
    }
   ],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "question = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"What is the capital of the Moon?\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedd201b",
   "metadata": {},
   "source": [
    "## Image input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e55fd5e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2e73e5dee6543529b3b37f428a16f05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FileUpload(value=(), accept='.png', description='Upload')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from ipywidgets import FileUpload\n",
    "from IPython.display import display\n",
    "\n",
    "uploader = FileUpload(accept=\".png\", multiple=False) # Use /files/dog.png\n",
    "display(uploader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1c2176b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "({'name': 'dog.png', 'type': 'image/png', 'size': 940294, 'content': <memory at 0x123b9aa40>, 'last_modified': datetime.datetime(2025, 12, 30, 17, 15, 25, 414000, tzinfo=datetime.timezone.utc)},)\n"
     ]
    }
   ],
   "source": [
    "print(uploader.value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39b9707",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "\n",
    "# Get the first (and only) uploaded file dict\n",
    "uploaded_file = uploader.value[0]\n",
    "\n",
    "# This is memoryview\n",
    "content_mv = uploaded_file[\"content\"]\n",
    "\n",
    "# Convert memoryview -> bytes\n",
    "img_bytes = bytes(content_mv) # or content_mv.tobytes()\n",
    "\n",
    "# Now base64 encode\n",
    "img_b64 = base64.b64encode(img_bytes).decode(\"utf-8\") # Convert raw bytes into base64 text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5ae60bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are the features I notice:\n",
      "\n",
      "- Size and stance: A small, compact dog lounging on a cushioned metal bench. Front paws rest over the edge, giving a relaxed, curious look.\n",
      "- Coat: Dense, tight curls all over the body. Solid black color with a bit of lighter gray around the muzzle and chin.\n",
      "- Face: Round, with fur that hides much of the eyes. Small, dark nose centered on the snout.\n",
      "- Ears: Folded into the fluffy fur; not clearly shaped or pointed.\n",
      "- Eyes: Dark and partly obscured by the hair, giving a softly scruffy expression.\n",
      "- Body build: Stocky and sturdy for a small dog; appears well-groomed.\n",
      "- Paws: Small and fluffy, proportionate to the body.\n",
      "- Tail: Fluffy and visible at the rear, blending into the rest of the coat.\n",
      "- Overall vibe: Calm, content, and a bit inquisitive, enjoying a sunny day on the bench.\n"
     ]
    }
   ],
   "source": [
    "multimodal_question = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"Tell me about the features you notice on this dog.\"},\n",
    "        {\"type\": \"image\", \"base64\": img_b64, \"mime_type\": \"image/png\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [multimodal_question]}\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ded67d",
   "metadata": {},
   "source": [
    "## Audio inout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713fa829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:05<00:00,  9.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import sounddevice as sd\n",
    "from scipy.io.wavfile import write\n",
    "import base64\n",
    "import io\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Recording settings\n",
    "duration = 5 # seconds\n",
    "sample_rate = 44100\n",
    "\n",
    "print(\"Recording...\")\n",
    "audio = sd.rec(int(duration * sample_rate), samplerate=sample_rate, channels=1)\n",
    "\n",
    "# Progress bar for the duration\n",
    "for _ in tqdm(range(duration * 10)): # Update 10x per second\n",
    "    time.sleep(0.1)\n",
    "\n",
    "sd.wait()\n",
    "print(\"Done.\")\n",
    "\n",
    "# Write WAV to an in-memory buffer\n",
    "buf = io.BytesIO()\n",
    "write(buf, sample_rate, audio)\n",
    "wav_bytes = buf.getvalue()\n",
    "\n",
    "aud_b64 = base64.b64encode(wav_bytes).decode(\"utf-8\") # Convert raw bytes into base64 text string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ccebace2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural networks have an interesting history that starts with the early exploration of how the human brain works. \n",
      "\n",
      "In 1943, Warren McCulloch and Walter Pitts published a paper that proposed a simplified model of neurons, essentially creating the first computational model of neural networks.\n",
      "\n",
      "In the 1950s, Frank Rosenblatt developed the Perceptron, which was an early type of neural network that could learn certain patterns. However, it wasn’t capable of solving more complex problems, which led to a decline in interest by the 1970s.\n",
      "\n",
      "In the 1980s, interest resurfaced with the development of new algorithms, like backpropagation, which allowed neural networks to adjust their weights and learn more effectively.\n",
      "\n",
      "By the 2000s and 2010s, with the advent of big data and more powerful computers, deep learning—using many-layered neural networks—became possible. This led to breakthroughs in tasks like image recognition, speech recognition, and natural language processing.\n",
      "\n",
      "Today, neural networks are a crucial part of modern AI applications across industries.\n"
     ]
    }
   ],
   "source": [
    "agent = create_agent(\n",
    "    model=\"gpt-4o-audio-preview\"\n",
    ")\n",
    "\n",
    "multimodal_question = HumanMessage(\n",
    "    content=[\n",
    "        {\"type\": \"text\", \"text\": \"Tell me about this audio file\"},\n",
    "        {\"type\": \"audio\", \"base64\": aud_b64, \"mime_type\": \"audio/wav\"}\n",
    "    ]\n",
    ")\n",
    "\n",
    "response = agent.invoke(\n",
    "    {\"messages\": [multimodal_question]}\n",
    ")\n",
    "\n",
    "print(response[\"messages\"][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
