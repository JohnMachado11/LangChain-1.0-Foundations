{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e591b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a41c51c",
   "metadata": {},
   "source": [
    "## Local MCP server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63fb0bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"local_server\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"resources/2.1_mcp_server.py\"]\n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c9d69f",
   "metadata": {},
   "source": [
    "https://modelcontextprotocol.io/specification/2025-11-25/basic/transports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec620baf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get tools\n",
    "tools = await client.get_tools()\n",
    "\n",
    "# Get resources\n",
    "resources = await client.get_resources(\"local_server\")\n",
    "\n",
    "# Get prompts\n",
    "prompt = await client.get_prompt(\"local_server\", \"prompt\")\n",
    "prompt = prompt[0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "77cd3348",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools,\n",
    "    system_prompt=prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15f755ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import HumanMessage\n",
    "\n",
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [HumanMessage(content=\"Tell me about the langchain-mcp-adapters library\")]},\n",
    "    config=config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7195e2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='Tell me about the langchain-mcp-adapters library', additional_kwargs={}, response_metadata={}, id='235358dd-b507-463b-87fc-8deabf518a8e'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 157, 'prompt_tokens': 270, 'total_tokens': 427, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 128, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Csc4nkKuCEVOQFYWTqJVx2UixT1GG', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b7148-296a-7593-b551-09d8f23f9d4c-0', tool_calls=[{'name': 'search_web', 'args': {'query': 'langchain-mcp-adapters library'}, 'id': 'call_GA9NVrW6y7uaBMP6oFx0aQsX', 'type': 'tool_call'}], usage_metadata={'input_tokens': 270, 'output_tokens': 157, 'total_tokens': 427, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 128}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"query\": \"langchain-mcp-adapters library\",\\n  \"follow_up_questions\": null,\\n  \"answer\": null,\\n  \"images\": [],\\n  \"results\": [\\n    {\\n      \"url\": \"https://github.com/langchain-ai/langchainjs-mcp-adapters\",\\n      \"title\": \"GitHub - langchain-ai/langchainjs-mcp-adapters: ** THIS ...\",\\n      \"content\": \"# Search code, repositories, users, issues, pull requests... You signed in with another tab or window. You signed out in another tab or window. You switched accounts on another tab or window. langchain-ai   /  **langchainjs-mcp-adapters**  Public archive. \\\\\\\\*\\\\\\\\* THIS REPO HAS MOVED TO  \\\\\\\\*\\\\\\\\* Adapters for integrating Model Context Protocol (MCP) tools with LangChain.js applications, supporting both stdio and SSE transports. 246 stars   34 forks   Branches   Tags   Activity. # langchain-ai/langchainjs-mcp-adapters. | RELEASE\\\\\\\\_NOTES.md | RELEASE\\\\\\\\_NOTES.md |  |  |. | tsconfig.cjs.json | tsconfig.cjs.json |  |  |. | tsconfig.examples.json | tsconfig.examples.json |  |  |. | tsconfig.tests.json | tsconfig.tests.json |  |  |. ## Repository files navigation. # LangChain.js MCP Adapters. This library provides a lightweight wrapper to allow Model Context Protocol (MCP) services to be used with LangChain.js. \\\\\\\\*\\\\\\\\* THIS REPO HAS MOVED TO  \\\\\\\\*\\\\\\\\* Adapters for integrating Model Context Protocol (MCP) tools with LangChain.js applications, supporting both stdio and SSE transports. javascript   typescript   mcp   ai-tools   langchain   llm-tools   openai-functions   langchainjs   llm-agents   agent-tools   llm-integration   model-context-protocol.\",\\n      \"score\": 0.885404,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph\",\\n      \"title\": \"MCP Adapters for LangChain and LangGraph\",\\n      \"content\": \"# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.\",\\n      \"score\": 0.8729662,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://pypi.org/project/langchain-mcp-adapters/\",\\n      \"title\": \"langchain-mcp-adapters\",\\n      \"content\": \"Make Anthropic Model Context Protocol (MCP) tools compatible with LangChain and LangGraph agents. * ðŸ› ï¸ Convert MCP tools into LangChain tools that can be used with LangGraph agents. * ðŸ“¦ A client implementation that allows you to connect to multiple MCP servers and load tools from them. Here is a simple example of using the MCP tools with a LangGraph agent. from langchain_mcp_adapters.tools import load_mcp_tools. The library also allows you to connect to multiple MCP servers and load tools from them:. from langchain_mcp_adapters.client import MultiServerMCPClient. > from langchain_mcp_adapters.tools import load_mcp_tools. from langchain_mcp_adapters.tools import load_mcp_tools. from langchain_mcp_adapters.client import MultiServerMCPClient. from langchain_mcp_adapters.client import MultiServerMCPClient. These headers are passed with every HTTP request to the MCP server. from langchain_mcp_adapters.client import MultiServerMCPClient. If you want to run a LangGraph agent that uses MCP tools in a LangGraph API server, you can use the following setup:. from langchain_mcp_adapters.client import MultiServerMCPClient. Details for the file `langchain_mcp_adapters-0.2.1.tar.gz`. Details for the file `langchain_mcp_adapters-0.2.1-py3-none-any.whl`. Hashes for langchain\\\\\\\\_mcp\\\\\\\\_adapters-0.2.1-py3-none-any.whl.\",\\n      \"score\": 0.8593928,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://reference.langchain.com/python/langchain_mcp_adapters/\",\\n      \"title\": \"langchain-mcp-adapters\",\\n      \"content\": \"# `langchain-mcp-adapters`Â¶. Client for connecting to multiple MCP servers and loading LC tools/resources. This module provides the `MultiServerMCPClient` class for managing connections to multiple MCP servers and loading tools, prompts, and resources from them. Loads LangChain-compatible tools, prompts and resources from MCP servers. | \\\\\\\\_\\\\\\\\_init\\\\\\\\_\\\\\\\\_ (`langchain_mcp_adapters.client.MultiServerMCPClient.__init__`)\\\\\">\\\\\\\\_\\\\\\\\_init\\\\\\\\_\\\\\\\\_ | Initialize a `MultiServerMCPClient` with MCP servers connections. | session  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.session`)\\\\\">session | Connect to an MCP server and initialize a session. | get\\\\\\\\_tools  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_tools`)\\\\\">get\\\\\\\\_tools | Get a list of all tools from all connected servers. | get\\\\\\\\_resources  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_resources`)\\\\\">get\\\\\\\\_resources | Get resources from MCP server(s). **TYPE:** `dict[str,`  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\\\\\">Connection] | None   **DEFAULT:** `None` |. ### load\\\\\\\\_mcp\\\\\\\\_tools `async` Â¶. (langchain_mcp_adapters.callbacks.Callbacks)\\\\\">Callbacks | None = None,  tool_interceptors: list[            ToolCallInterceptor (langchain_mcp_adapters.interceptors.ToolCallInterceptor)\\\\\">ToolCallInterceptor] | None = None,  server_name: str | None = None,  tool_name_prefix: bool = False, ) -> list[            BaseTool (langchain_core.tools.BaseTool)\\\\\">BaseTool]. **TYPE:**  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\\\\\">Connection | None   **DEFAULT:** `None` |. ### load\\\\\\\\_mcp\\\\\\\\_resources `async` Â¶. | \\\\\\\\_\\\\\\\\_call\\\\\\\\_\\\\\\\\_  `async`  (`langchain_mcp_adapters.interceptors.ToolCallInterceptor.__call__`)\\\\\">\\\\\\\\_\\\\\\\\_call\\\\\\\\_\\\\\\\\_ | Intercept tool execution with control over handler invocation.\",\\n      \"score\": 0.8576849,\\n      \"raw_content\": null\\n    },\\n    {\\n      \"url\": \"https://docs.langchain.com/oss/javascript/langchain/mcp\",\\n      \"title\": \"Model Context Protocol (MCP) - Docs by LangChain\",\\n      \"content\": \"Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the `@langchain/mcp-adapters` library. import { MultiServerMCPClient } from \\\\\"@langchain/mcp-adapters\\\\\"; import { MultiServerMCPClient } from \\\\\"@langchain/mcp-adapters\\\\\"; import { ChatAnthropic } from \\\\\"@langchain/anthropic\\\\\"; import { ChatAnthropic } from \\\\\"@langchain/anthropic\\\\\";import { createAgent } from \\\\\"langchain\\\\\"; import { createAgent } from  \\\\\"langchain\\\\\"; const client = new MultiServerMCPClient({ const  client =  new  MultiServerMCPClient({  math: { math: { transport: \\\\\"stdio\\\\\", // Local subprocess communication transport:  \\\\\"stdio\\\\\", // Local subprocess communication command: \\\\\"node\\\\\", command:  \\\\\"node\\\\\", // Replace with absolute path to your math_server.js file // Replace with absolute path to your math_server.js file args: [\\\\\"/path/to/math_server.js\\\\\"], args: [\\\\\"/path/to/math_server.js\\\\\"], }, }, weather: { weather: { transport: \\\\\"http\\\\\", // HTTP-based remote server transport:  \\\\\"http\\\\\", // HTTP-based remote server // Ensure you start your weather server on port 8000 // Ensure you start your weather server on port 8000 url: \\\\\"http://localhost:8000/mcp\\\\\", url: \\\\\"http://localhost:8000/mcp\\\\\", }, },});}); const tools = await client.getTools(); const  tools =  await  client.\",\\n      \"score\": 0.8497846,\\n      \"raw_content\": null\\n    }\\n  ],\\n  \"response_time\": 0.0,\\n  \"request_id\": \"554101fb-1901-495a-b7d8-7b23340da3a4\"\\n}', 'id': 'lc_3b8dfc13-72be-4a85-9827-8491041c0bd8'}], name='search_web', id='dc4de1cd-933d-48b9-90c5-4bd7d36039d1', tool_call_id='call_GA9NVrW6y7uaBMP6oFx0aQsX', artifact={'structured_content': {'result': {'query': 'langchain-mcp-adapters library', 'follow_up_questions': None, 'answer': None, 'images': [], 'results': [{'url': 'https://github.com/langchain-ai/langchainjs-mcp-adapters', 'title': 'GitHub - langchain-ai/langchainjs-mcp-adapters: ** THIS ...', 'content': '# Search code, repositories, users, issues, pull requests... You signed in with another tab or window. You signed out in another tab or window. You switched accounts on another tab or window. langchain-ai   /  **langchainjs-mcp-adapters**  Public archive. \\\\*\\\\* THIS REPO HAS MOVED TO  \\\\*\\\\* Adapters for integrating Model Context Protocol (MCP) tools with LangChain.js applications, supporting both stdio and SSE transports. 246 stars   34 forks   Branches   Tags   Activity. # langchain-ai/langchainjs-mcp-adapters. | RELEASE\\\\_NOTES.md | RELEASE\\\\_NOTES.md |  |  |. | tsconfig.cjs.json | tsconfig.cjs.json |  |  |. | tsconfig.examples.json | tsconfig.examples.json |  |  |. | tsconfig.tests.json | tsconfig.tests.json |  |  |. ## Repository files navigation. # LangChain.js MCP Adapters. This library provides a lightweight wrapper to allow Model Context Protocol (MCP) services to be used with LangChain.js. \\\\*\\\\* THIS REPO HAS MOVED TO  \\\\*\\\\* Adapters for integrating Model Context Protocol (MCP) tools with LangChain.js applications, supporting both stdio and SSE transports. javascript   typescript   mcp   ai-tools   langchain   llm-tools   openai-functions   langchainjs   llm-agents   agent-tools   llm-integration   model-context-protocol.', 'score': 0.885404, 'raw_content': None}, {'url': 'https://changelog.langchain.com/announcements/mcp-adapters-for-langchain-and-langgraph', 'title': 'MCP Adapters for LangChain and LangGraph', 'content': '# LangChain Changelog. Sign up for our newsletter to stay up to date. # MCP Adapters for LangChain and LangGraph. The **LangChain MCP Adapters** is a package that makes it easy to use **Anthropic Model Context Protocol (MCP) tools** with LangChain & LangGraph. * Converts **MCP tools** into **LangChain- & LangGraph-compatible tools**. * Enables interaction with tools across multiple **MCP servers**. * Seamlessly integrates the **hundreds of tool servers** already published into LangGraph Agents. ### Why use MCP Adapters:. This adapter makes it simple to connect LangChain and LangGraph with the growing ecosystem of MCP tool servers. Instead of manually adapting each tool, you can now integrate them seamlessly. It also allows agents to pull from multiple MCP servers at once, making it easier to combine different tools for more powerful applications. MCP is gaining serious traction, and this adapter helps LangGraph agents take full advantage. ##### Subscribe to updates.', 'score': 0.8729662, 'raw_content': None}, {'url': 'https://pypi.org/project/langchain-mcp-adapters/', 'title': 'langchain-mcp-adapters', 'content': 'Make Anthropic Model Context Protocol (MCP) tools compatible with LangChain and LangGraph agents. * ðŸ› ï¸ Convert MCP tools into LangChain tools that can be used with LangGraph agents. * ðŸ“¦ A client implementation that allows you to connect to multiple MCP servers and load tools from them. Here is a simple example of using the MCP tools with a LangGraph agent. from langchain_mcp_adapters.tools import load_mcp_tools. The library also allows you to connect to multiple MCP servers and load tools from them:. from langchain_mcp_adapters.client import MultiServerMCPClient. > from langchain_mcp_adapters.tools import load_mcp_tools. from langchain_mcp_adapters.tools import load_mcp_tools. from langchain_mcp_adapters.client import MultiServerMCPClient. from langchain_mcp_adapters.client import MultiServerMCPClient. These headers are passed with every HTTP request to the MCP server. from langchain_mcp_adapters.client import MultiServerMCPClient. If you want to run a LangGraph agent that uses MCP tools in a LangGraph API server, you can use the following setup:. from langchain_mcp_adapters.client import MultiServerMCPClient. Details for the file `langchain_mcp_adapters-0.2.1.tar.gz`. Details for the file `langchain_mcp_adapters-0.2.1-py3-none-any.whl`. Hashes for langchain\\\\_mcp\\\\_adapters-0.2.1-py3-none-any.whl.', 'score': 0.8593928, 'raw_content': None}, {'url': 'https://reference.langchain.com/python/langchain_mcp_adapters/', 'title': 'langchain-mcp-adapters', 'content': '# `langchain-mcp-adapters`Â¶. Client for connecting to multiple MCP servers and loading LC tools/resources. This module provides the `MultiServerMCPClient` class for managing connections to multiple MCP servers and loading tools, prompts, and resources from them. Loads LangChain-compatible tools, prompts and resources from MCP servers. | \\\\_\\\\_init\\\\_\\\\_ (`langchain_mcp_adapters.client.MultiServerMCPClient.__init__`)\">\\\\_\\\\_init\\\\_\\\\_ | Initialize a `MultiServerMCPClient` with MCP servers connections. | session  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.session`)\">session | Connect to an MCP server and initialize a session. | get\\\\_tools  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_tools`)\">get\\\\_tools | Get a list of all tools from all connected servers. | get\\\\_resources  `async`  (`langchain_mcp_adapters.client.MultiServerMCPClient.get_resources`)\">get\\\\_resources | Get resources from MCP server(s). **TYPE:** `dict[str,`  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\">Connection] | None   **DEFAULT:** `None` |. ### load\\\\_mcp\\\\_tools `async` Â¶. (langchain_mcp_adapters.callbacks.Callbacks)\">Callbacks | None = None,  tool_interceptors: list[            ToolCallInterceptor (langchain_mcp_adapters.interceptors.ToolCallInterceptor)\">ToolCallInterceptor] | None = None,  server_name: str | None = None,  tool_name_prefix: bool = False, ) -> list[            BaseTool (langchain_core.tools.BaseTool)\">BaseTool]. **TYPE:**  Connection  `module-attribute`  (`langchain_mcp_adapters.sessions.Connection`)\">Connection | None   **DEFAULT:** `None` |. ### load\\\\_mcp\\\\_resources `async` Â¶. | \\\\_\\\\_call\\\\_\\\\_  `async`  (`langchain_mcp_adapters.interceptors.ToolCallInterceptor.__call__`)\">\\\\_\\\\_call\\\\_\\\\_ | Intercept tool execution with control over handler invocation.', 'score': 0.8576849, 'raw_content': None}, {'url': 'https://docs.langchain.com/oss/javascript/langchain/mcp', 'title': 'Model Context Protocol (MCP) - Docs by LangChain', 'content': 'Model Context Protocol (MCP) is an open protocol that standardizes how applications provide tools and context to LLMs. LangChain agents can use tools defined on MCP servers using the `@langchain/mcp-adapters` library. import { MultiServerMCPClient } from \"@langchain/mcp-adapters\"; import { MultiServerMCPClient } from \"@langchain/mcp-adapters\"; import { ChatAnthropic } from \"@langchain/anthropic\"; import { ChatAnthropic } from \"@langchain/anthropic\";import { createAgent } from \"langchain\"; import { createAgent } from  \"langchain\"; const client = new MultiServerMCPClient({ const  client =  new  MultiServerMCPClient({  math: { math: { transport: \"stdio\", // Local subprocess communication transport:  \"stdio\", // Local subprocess communication command: \"node\", command:  \"node\", // Replace with absolute path to your math_server.js file // Replace with absolute path to your math_server.js file args: [\"/path/to/math_server.js\"], args: [\"/path/to/math_server.js\"], }, }, weather: { weather: { transport: \"http\", // HTTP-based remote server transport:  \"http\", // HTTP-based remote server // Ensure you start your weather server on port 8000 // Ensure you start your weather server on port 8000 url: \"http://localhost:8000/mcp\", url: \"http://localhost:8000/mcp\", }, },});}); const tools = await client.getTools(); const  tools =  await  client.', 'score': 0.8497846, 'raw_content': None}], 'response_time': 0.0, 'request_id': '554101fb-1901-495a-b7d8-7b23340da3a4'}}}),\n",
      "              AIMessage(content='Hereâ€™s a concise overview of the langchain-mcp-adapters library and what itâ€™s for.\\n\\nWhat MCP adapters are\\n- MCP stands for Model Context Protocol. Itâ€™s a model-tooling protocol that standardizes how tools and context are provided to LLMs.\\n- The MCP adapters package helps you use MCP tools with LangChain and LangGraph by converting MCP tools into LangChain/LangGraph-compatible tools. It also supports interacting with multiple MCP servers from a single client.\\n\\nWhat the library provides\\n- A bridge between MCP tool servers and LangChain/LangGraph agents\\n  - Converts MCP tools into LangChain tools (so they can be used by LangChain agents and LangGraph workflows)\\n  - Enables agents to pull from multiple MCP servers at once\\n- Multi-server client\\n  - Python: MultiServerMCPClient to manage connections to several MCP servers and load tools/resources\\n  - JavaScript: langchainjs-mcp-adapters (JS version) for LangChain.js\\n- Loading tools and resources\\n  - Tools, prompts, and other resources hosted on MCP servers can be loaded and used inside LangChain/LangGraph\\n- Transports supported\\n  - Local/stdio (for running MCP servers as subprocesses)\\n  - Server-Sent Events (SSE) or HTTP transports for remote MCP servers\\n- Request headers\\n  - The HTTP requests to MCP servers can include headers (e.g., for authentication)\\n\\nKey sources you can check\\n- PyPI: langchain-mcp-adapters (Python)\\n  - Describes converting MCP tools to LangChain tools, loading tools from multiple MCP servers, and examples\\n  - Use: MultiServerMCPClient to connect to servers; load_mcp_tools to load tools/prompts/resources\\n- LangChain docs (Python)\\n  - Reference page for langchain_mcp_adapters (MultiServerMCPClient, loading tools/resources, etc.)\\n- GitHub (JS): langchainjs-mcp-adapters\\n  - The JavaScript/TypeScript version for LangChain.js; adapter library for MCP with LangChain.js\\n- LangChain changelog article\\n  - Announcement and overview of MCP Adapters for LangChain and LangGraph, including capabilities and benefits\\n- Additional docs page (MCP in LangChain)\\n  - General MCP integration guidance and how to use MCP tools with LangChain/ LangGraph\\n\\nSimple usage outline (high level)\\n- Python (example concepts; adapt to your setup)\\n  - Create a MultiServerMCPClient with your MCP server definitions (HTTP as transport, or stdio for local subprocess servers)\\n  - Optionally open a session to the MCP server(s)\\n  - Retrieve tools via get_tools or load them via load_mcp_tools\\n  - Use the loaded LangChain-compatible tools with your LangChain/LangGraph agents\\n\\n- JavaScript/TypeScript (LangChain.js)\\n  - Import MultiServerMCPClient from @langchain/mcp-adapters\\n  - Instantiate it with your MCP server configurations (HTTP URL or stdio transports)\\n  - Call getTools to fetch available tools, or use them directly in agents\\n\\nWhy youâ€™d use it\\n- You already have an ecosystem of MCP tool servers and want to leverage them inside LangChain/LangGraph without manually adapting each tool\\n- You need to orchestrate tools from multiple MCP servers in a single LangChain/LangGraph workflow\\n- Youâ€™re integrating MCP tools into an agent-based system and want seamless compatibility with LangChainâ€™s tooling framework\\n\\nIf youâ€™d like, I can:\\n- Show a concrete quickstart example in Python or JS with a minimal MCP server configuration\\n- Point you to a specific section in the PyPI page or the LangChain docs for exact function signatures and code samples\\n- Help you decide whether to use the Python or JavaScript adapter based on your tech stack\\n\\nWould you like a short code example in Python or JavaScript to get a few MCP tools loaded and ready to use with a LangChain agent?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 2740, 'prompt_tokens': 2383, 'total_tokens': 5123, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 1920, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Csc4rXId2QrETaBkJ5gXBH7jBB602', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b7148-36f8-7e62-a10e-ee94d4da8232-0', usage_metadata={'input_tokens': 2383, 'output_tokens': 2740, 'total_tokens': 5123, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 1920}})]}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "pprint(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c72141",
   "metadata": {},
   "source": [
    "## Online MCP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ad679c",
   "metadata": {},
   "source": [
    "https://mcp.so/servers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9ce6273",
   "metadata": {},
   "source": [
    "MCP Server used below:\n",
    "\n",
    "https://mcp.so/server/time/modelcontextprotocol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7832137c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "client = MultiServerMCPClient(\n",
    "    {\n",
    "        \"time\": {\n",
    "            \"transport\": \"stdio\",\n",
    "            \"command\": sys.executable, # uses the same Python as this notebook kernel\n",
    "            \"args\": [\n",
    "                \"-m\",\n",
    "                \"mcp_server_time\",\n",
    "                \"--local-timezone=America/New_York\"\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b6e0c116",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[StructuredTool(name='get_current_time', description='Get current time in a specific timezones', args_schema={'type': 'object', 'properties': {'timezone': {'type': 'string', 'description': \"IANA timezone name (e.g., 'America/New_York', 'Europe/London'). Use 'America/New_York' as local timezone if no timezone provided by the user.\"}}, 'required': ['timezone']}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x11e23cae0>),\n",
       " StructuredTool(name='convert_time', description='Convert time between timezones', args_schema={'type': 'object', 'properties': {'source_timezone': {'type': 'string', 'description': \"Source IANA timezone name (e.g., 'America/New_York', 'Europe/London'). Use 'America/New_York' as local timezone if no source timezone provided by the user.\"}, 'time': {'type': 'string', 'description': 'Time to convert in 24-hour format (HH:MM)'}, 'target_timezone': {'type': 'string', 'description': \"Target IANA timezone name (e.g., 'Asia/Tokyo', 'America/San_Francisco'). Use 'America/New_York' as local timezone if no target timezone provided by the user.\"}}, 'required': ['source_timezone', 'time', 'target_timezone']}, response_format='content_and_artifact', coroutine=<function convert_mcp_tool_to_langchain_tool.<locals>.call_tool at 0x11c411760>)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6622abf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_agent(\n",
    "    model=\"gpt-5-nano\",\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c8196a32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'messages': [HumanMessage(content='What time is it?', additional_kwargs={}, response_metadata={}, id='a92d1185-7e51-4e56-bf05-0e75fd8617e2'),\n",
      "              AIMessage(content='', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 283, 'prompt_tokens': 296, 'total_tokens': 579, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 256, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Csc59peWaFLdc8rnXracYugjh7AsA', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='lc_run--019b7148-7f9f-78d2-aa23-5565372eaf29-0', tool_calls=[{'name': 'get_current_time', 'args': {'timezone': 'America/New_York'}, 'id': 'call_IYcYoJz29Ua2LepCvzOldfVH', 'type': 'tool_call'}], usage_metadata={'input_tokens': 296, 'output_tokens': 283, 'total_tokens': 579, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 256}}),\n",
      "              ToolMessage(content=[{'type': 'text', 'text': '{\\n  \"timezone\": \"America/New_York\",\\n  \"datetime\": \"2025-12-30T17:02:19-05:00\",\\n  \"day_of_week\": \"Tuesday\",\\n  \"is_dst\": false\\n}', 'id': 'lc_c4259321-b217-4ec1-b158-193e96807fcc'}], name='get_current_time', id='97c3a981-db0b-4cc1-af7f-81b725b31c3b', tool_call_id='call_IYcYoJz29Ua2LepCvzOldfVH'),\n",
      "              AIMessage(content='Current time in New York (Eastern Time): 5:02:19 PM on Tuesday, December 30, 2025 (EST, UTC-5). \\n\\nIf you want this in another time zone, I can convert it.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 441, 'prompt_tokens': 379, 'total_tokens': 820, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 384, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-5-nano-2025-08-07', 'system_fingerprint': None, 'id': 'chatcmpl-Csc5DP9QqAWnLLUWPjqyIcgCp1za3', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b7148-9134-71d0-bdd6-51aa999e7b0b-0', usage_metadata={'input_tokens': 379, 'output_tokens': 441, 'total_tokens': 820, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 384}})]}\n"
     ]
    }
   ],
   "source": [
    "question = HumanMessage(content=\"What time is it?\")\n",
    "\n",
    "response = await agent.ainvoke(\n",
    "    {\"messages\": [question]}\n",
    ")\n",
    "\n",
    "pprint(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
